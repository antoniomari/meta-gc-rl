name:  &job_name size-lr-ablation
results_dir: /fast/malbaba/results/offline_finetuning
git_branch: main
git_commit: null
script_path: main.py
pre_script: null
cluster_requirements:
  cpus: 8
  ram: 32G
  gpus: 1
  disk: 64GB
  bid: 25
  forbidden_hostnames: ['g054', 'g156', 'i206']
  time: 24:00:00
default_params: default.yaml
grid:
  run_group: *job_name
  train_steps: 1000001
  eval_interval: 100000
  eval_start: 800000
  log_interval: 50000
  save_interval: 50000
  eval_on_cpu: 0
  eval_episodes: 3
  '[env_name, agent.agent_name, agent.actor_loss, finetune.actor_loss, agent.alpha, agent.actor_p_randomgoal, agent.actor_p_trajgoal]': [
    [antmaze-medium-stitch-v0, gciql, ddpgbc, ddpgbc, 0.3, 0.5, 0.5],
  ]

  agent.high_alpha: 3.0
  agent.low_alpha: 3.0
  #agent.alpha: 0.3

  finetune.ratio: 1.0
  finetune.fix_actor_goal: 1.0
  finetune.filter_by_td: false
  finetune.mc_quantile: 0.2 #0.2
  finetune.mc_slack: 10 #10 # might change this to 5
  finetune.sorb_len: 5 # not important for recursive
  finetune.td_correction_horizon: 20
  finetune.td_quantile: 0.2
  finetune.filter_by_argmax: false
  finetune.filter_by_nstep_td: false

  finetune.max_steps: 15
  finetune.min_steps: 10
  finetune.mc_similarity_threshold: 1.0
  # agent.lr: 3e-3, 3e-5
  agent.lr: [3.e-3, 3.e-5]

  '[agent.actor_hidden_dims, agent.value_hidden_dims]': [
    [[512, 512, 512], [512, 512, 512]],
    [[624, 624, 624], [624, 624, 624]], # 
    [[732, 732, 732], [732, 732, 732]],
    [[992, 992, 992], [992, 992, 992]],
    [[1332, 1332, 1332], [1332, 1332, 1332]]
  ]

  '[finetune.lr, finetune.num_steps, finetune.filter_by_mc, finetune.filter_by_recursive_mdp, finetune.replan_horizon]': [
    [0, 0, false, false, 100],
  ]

  seed: [0, 1, 2]